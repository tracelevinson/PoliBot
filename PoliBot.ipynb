{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>The PoliBot: Keeping Us Current & Unbiased on American Politics</center></h1>\n",
    "<img src=\"images/robot-voter-dreamstime-image.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<figcaption><center>Photo Credit: Dreamstime</center></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Summary\n",
    "\n",
    "The PoliBot (@thePoliBot) is an NLP-based chatbot that provides recommendations of current political news articles based on interactive user input. For example, a user can give thePoliBot queries such as: \"Why did Donald Trump fire Comey?\" or \"Facebook's role in Russian meddling in the 2016 election\", and it will respond with recent relevant news articles from 15 major news sources. This chatbot can be seen as a domain-specific search engine with suggested results based purely on relevance to the user query, exposing users to a broader set of media sources than they may normally follow. It also has a general dialogue component, and will decipher when the user asks for political information versus simply wanting to chat. The dialogue component is trained on the Cornell movie dialogues dataset, with the PoliBot responding in simple playful dialogue when chatting. The bot was deployed in the Telegram app, running in a Docker container on an AWS EC2 server instance (ubuntu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the current unpreditability in the U.S. political system, it can be nearly impossible to stay ahead of the curve on the present state of things. Furthermore, the tendency for most of us to follow only a select few news sources has contributed to an increasingly polarized political environment, as we often reinforce our previous views in an attempt to flee from fake news. These joint trends fueled my motivation to create the PoliBot: a fun, interactive tool that provides news updates from a variety of different viewpoints to broaden the context in which we digest current events. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Data\n",
    "\n",
    "The bot is trained using two different datasets: a set of general dialogues and a set of political article texts. Each dataset is used to train on two different models. The general dialogue dataset will be used to: (1) train on responses to apolitical user input (general chit-chat), and (2) to classify user intent between political queries and chit-chat. The political article dataset will be used for: (1) the same intent classifier, and (2) a model to create article embeddings for the generation of query-response article recommendations. The following section describes each of these  models in detail. \n",
    "\n",
    "The dialogue dataset is pulled directly from the Cornell movie dialogues dataset, which includes 220,579 utterances across a variety of movie genres and characters. The dataset has been preprocessed from another project and is imported as a clean TSV file.\n",
    "\n",
    "The political article dataset is hand accumulated using Selenium, BeautifulSoup, and Lynx to web scrape a final set of 44,193 articles across 15 major media sources. Those represented include: CNN, Fox News, Vox, New York Times, Washington Post, NPR, Los Angeles Times, Wall Street Journal, MSNBC, BBC, Politico, Reuters, USA Today, Chicago Tribune, and Newsweek. Articles are heavily concentrated within the last six months (late 2017 - April 2018), with rare exceptions extending as far back as October 2016. These are restricted to the domain of U.S. politics, given by the search specifications in *source\\_urls.csv*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Data Accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing a set of utility functions that facilitate data extraction, cleaning, manipulation, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article links are gathered top down using search filters on each media source homepage. There are two primary methods for pulling in URLs: xpath and Lynx. The extraction method for each source is determined by each website format. For those that expose their embedded URLs as visible links, Lynx proves more effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exec(open('./extract_urls.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Data Scrubbing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon pulling in a massive set of all potential URLs, there are many irrelevant articles and other links that must be removed from the final dataset. To account for this, a separate script compiles a clean dataset of only relevant article links from the raw input. Each news source contains a particular regular expression that determines pertinent text article URLs. These regex patterns are compared against all links to obtain a final set of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exec(open('./clean_urls.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With a clean set of article URLs, it is time to extract the body text from each article for analysis. This process involves the extraction of HTML tags specific to each news source. Through a deep dive into the HTML structure of each site, I created a dictionary detailing the corresponding paths to extract from each link to form the resulting full dataset with text.\n",
    "\n",
    "The text from this full dataset is then cleaned and pre-processed for easy modeling use. The final dataset is reformatted to remove duplicates and facilitate data frame exportation to TSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           num_articles  median_word_count\n",
      "bbc                 346                293\n",
      "chicago            4849                537\n",
      "cnn                6317                330\n",
      "fox_news           2035                330\n",
      "la_times           1418                535\n",
      "msnbc              3996                389\n",
      "newsweek           4084                323\n",
      "npr                3963                484\n",
      "ny_times           2189                632\n",
      "politico           2566                975\n",
      "reuters            5000                242\n",
      "usa_today          2393                456\n",
      "vox                1062                605\n",
      "wapo                536                487\n",
      "wsj                3386                468\n"
     ]
    }
   ],
   "source": [
    "exec(open('./scrape_url_text.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two primary models are trained for chatbot functioning. The first determines which articles are recommended to the user given her input query. The second, which is actually called first during bot interaction, classifies user input as either a political query or general dialogue to guide the bot's response toward article recommendations or chit-chat, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Starspace Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the user inputs a political query that warrants article recommendations, the bot will need to assess the most relevant articles to recommend. This task is accomplished using the ultra-convenient Starspace package distributed by Facebook's research group. The model takes in the set of compiled article texts as input and returns embeddings for each article as output. \n",
    "\n",
    "For the purposes of this project, I use training mode 2 to establish word embeddings across all articles. This training mode effectively transforms a set of unsupervised training data (the body of text from each article) into supervised training examples. For each observation (article), one sentence is chosen at random as input, the remaining sentences of that article are used as a positive label, and a subset of other articles' text are randomly chosen as negative examples of association with the input sentence. Ultimately, embeddings for article vocabulary words are trained to optimize cosine similarity between sentences within the same article. These embeddings are then extended to rank similarity between user queries and articles at test time to provide article recommendations. \n",
    "\n",
    "For more in-depth information on Starspace embeddings, see the [GitHub](https://github.com/facebookresearch/StarSpace) and [ArXiV paper](https://arxiv.org/pdf/1709.03856.pdf), specifically the ArticleSpace sections.\n",
    "\n",
    "This script prepares Starspace training and test data, then calls a bash script to train, test, and evaluate the model on Linux. Final document-level embeddings are computed as the mean of all word embeddings within each article's text. Article recommendations would be definitely be improved by using a more sophisticated aggregation method than this simple mean. \n",
    "\n",
    "The output is stored together with article URL and source information for bot use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exec(open('./build_starspace.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Intent Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second, heartier model deciphers user input between political queries and general dialogue. This is structured as a binary text classification problem. I choose a bidirectional LSTM neural network with dropout here for its ability to track seemingly convoluted and extensive sequences that may arise with user input. The LSTM employs an Adam optimizer with a sigmoid cross entropy loss function.\n",
    "\n",
    "The intents dataset is first prepared for modeling. Political text samples are taken directly from the article text dataset used in the embeddings model, splitting text into one sentence per observation. These samples are then combined with the Cornell movie dialogues dataset to represent general dialogue. While these dialogue data do not exhaustively represent potential user input, they span a wide enough range of topics and contexts to be effective. The dataset is then prepared for analysis by pre-processing text, capping input at 200 words for outlier removal and efficiency, and adding 'political' and 'dialogue' labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exec(open('./intents_prep.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dataset size comes to approximately 400,000 observations, balanced evenly between 'political' and 'dialogue' samples. These are split into 80/10/10 training, validation, and test sets for modeling. \n",
    "\n",
    "Dictionaries mapping between words and integer values are created, enabling each input batch of sentences to be converted to numerical LSTM inputs. The model is then implemented in TensorFlow and embedded within an IntentClassifier class object. After 10 epochs, the model achieves high performance with nearly 93% test accuracy. \n",
    "\n",
    "Note: The training and validation losses shown below derive from randomly compiled input batches, and therefore will not decrease monotonically. LSTM dropout also yields a slightly different network with each iteration, leading to further fluctuation in loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training: \n",
      "\n",
      "Training epoch 1\n",
      "Epoch: [1/10], step: [1/2486], loss: 0.696802\n",
      "Epoch: [1/10], step: [401/2486], loss: 0.365114\n",
      "Epoch: [1/10], step: [801/2486], loss: 0.366471\n",
      "Epoch: [1/10], step: [1201/2486], loss: 0.356616\n",
      "Epoch: [1/10], step: [1601/2486], loss: 0.360487\n",
      "Epoch: [1/10], step: [2001/2486], loss: 0.310928\n",
      "Epoch: [1/10], step: [2401/2486], loss: 0.297060\n",
      "Validation epoch 1 loss: 0.21858135 \n",
      "\n",
      "X: copyright 2017 npr # # # # # # # # # # # # # # # # # # #\n",
      "Y: True\n",
      "P: 0.9978624 \n",
      "\n",
      "X: everyone wants legalize weed # # # # # # # # # # # # # # # # # #\n",
      "Y: True\n",
      "P: 0.39922687 \n",
      "\n",
      "X: wrote first book used carry around looking publisher good book marcia writer # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.44563374 \n",
      "\n",
      "Training epoch 2\n",
      "Epoch: [2/10], step: [401/2486], loss: 0.263141\n",
      "Epoch: [2/10], step: [801/2486], loss: 0.281445\n",
      "Epoch: [2/10], step: [1201/2486], loss: 0.283057\n",
      "Epoch: [2/10], step: [1601/2486], loss: 0.219097\n",
      "Epoch: [2/10], step: [2001/2486], loss: 0.294121\n",
      "Epoch: [2/10], step: [2401/2486], loss: 0.167343\n",
      "Validation epoch 2 loss: 0.17646335 \n",
      "\n",
      "X: importance womens votes voice first consider womens right vote # # # # # # # # # # # # # #\n",
      "Y: True\n",
      "P: 0.9896515 \n",
      "\n",
      "X: era stark partisanship common thread serves reminder consistent message state union # # # # # # # # # # # #\n",
      "Y: True\n",
      "P: 0.99140805 \n",
      "\n",
      "X: cy cy oh shit man pigs # # # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.006768302 \n",
      "\n",
      "Training epoch 3\n",
      "Epoch: [3/10], step: [401/2486], loss: 0.232992\n",
      "Epoch: [3/10], step: [801/2486], loss: 0.191195\n",
      "Epoch: [3/10], step: [1201/2486], loss: 0.252945\n",
      "Epoch: [3/10], step: [1601/2486], loss: 0.126118\n",
      "Epoch: [3/10], step: [2001/2486], loss: 0.223080\n",
      "Epoch: [3/10], step: [2401/2486], loss: 0.159255\n",
      "Validation epoch 3 loss: 0.25728077 \n",
      "\n",
      "X: guess good mine mate m another matter entirely # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.0082290545 \n",
      "\n",
      "X: nt way catch time # # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.019468041 \n",
      "\n",
      "X: nothing ever really changes david # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.26056996 \n",
      "\n",
      "Training epoch 4\n",
      "Epoch: [4/10], step: [401/2486], loss: 0.176448\n",
      "Epoch: [4/10], step: [801/2486], loss: 0.190235\n",
      "Epoch: [4/10], step: [1201/2486], loss: 0.179750\n",
      "Epoch: [4/10], step: [1601/2486], loss: 0.262535\n",
      "Epoch: [4/10], step: [2001/2486], loss: 0.158613\n",
      "Epoch: [4/10], step: [2401/2486], loss: 0.219362\n",
      "Validation epoch 4 loss: 0.26518768 \n",
      "\n",
      "X: ve got republicans right nt like bill certain reasons # # # # # # # # # # # # #\n",
      "Y: True\n",
      "P: 0.8543184 \n",
      "\n",
      "X: hate see situation # # # # # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.11585891 \n",
      "\n",
      "X: home one alabama republican roy moores accusers burned week arson investigation underway # # # # # # # # # #\n",
      "Y: True\n",
      "P: 0.99642855 \n",
      "\n",
      "Training epoch 5\n",
      "Epoch: [5/10], step: [401/2486], loss: 0.139102\n",
      "Epoch: [5/10], step: [801/2486], loss: 0.139601\n",
      "Epoch: [5/10], step: [1201/2486], loss: 0.156799\n",
      "Epoch: [5/10], step: [1601/2486], loss: 0.188312\n",
      "Epoch: [5/10], step: [2001/2486], loss: 0.146419\n",
      "Epoch: [5/10], step: [2401/2486], loss: 0.156664\n",
      "Validation epoch 5 loss: 0.13633496 \n",
      "\n",
      "X: must safety security together strong military great people # # # # # # # # # # # #\n",
      "Y: True\n",
      "P: 0.71320146 \n",
      "\n",
      "X: promise wo nt forget # # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.028578008 \n",
      "\n",
      "X: think could still town # # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.092054166 \n",
      "\n",
      "Training epoch 6\n",
      "Epoch: [6/10], step: [401/2486], loss: 0.203837\n",
      "Epoch: [6/10], step: [801/2486], loss: 0.094464\n",
      "Epoch: [6/10], step: [1201/2486], loss: 0.115000\n",
      "Epoch: [6/10], step: [1601/2486], loss: 0.228585\n",
      "Epoch: [6/10], step: [2001/2486], loss: 0.224053\n",
      "Epoch: [6/10], step: [2401/2486], loss: 0.243757\n",
      "Validation epoch 6 loss: 0.16727778 \n",
      "\n",
      "X: shower s # # # # # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.10658473 \n",
      "\n",
      "X: hey say # # # # # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.38454768 \n",
      "\n",
      "X: - donald j trump realdonaldtrump july 23 2012 cases trump s calls went unanswered # # # # # # #\n",
      "Y: True\n",
      "P: 0.99998355 \n",
      "\n",
      "Training epoch 7\n",
      "Epoch: [7/10], step: [401/2486], loss: 0.269464\n",
      "Epoch: [7/10], step: [801/2486], loss: 0.147370\n",
      "Epoch: [7/10], step: [1201/2486], loss: 0.202699\n",
      "Epoch: [7/10], step: [1601/2486], loss: 0.155289\n",
      "Epoch: [7/10], step: [2001/2486], loss: 0.157171\n",
      "Epoch: [7/10], step: [2401/2486], loss: 0.172821\n",
      "Validation epoch 7 loss: 0.11704404 \n",
      "\n",
      "X: oh help balloon s going # # # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.004312984 \n",
      "\n",
      "X: s high-risk high-liability proposition says school security consultant ken trump relation president # # # # # # # # # #\n",
      "Y: True\n",
      "P: 0.99995327 \n",
      "\n",
      "X: right right s fairly good high side tried nuts # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.0045425394 \n",
      "\n",
      "Training epoch 8\n",
      "Epoch: [8/10], step: [401/2486], loss: 0.154455\n",
      "Epoch: [8/10], step: [801/2486], loss: 0.134221\n",
      "Epoch: [8/10], step: [1201/2486], loss: 0.126788\n",
      "Epoch: [8/10], step: [1601/2486], loss: 0.236606\n",
      "Epoch: [8/10], step: [2001/2486], loss: 0.178301\n",
      "Epoch: [8/10], step: [2401/2486], loss: 0.193852\n",
      "Validation epoch 8 loss: 0.09103581 \n",
      "\n",
      "X: donald trump re-elected 2020 # # # # # # # # # # # # # # # # # #\n",
      "Y: True\n",
      "P: 0.9999912 \n",
      "\n",
      "X: find # # # # # # # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.10142997 \n",
      "\n",
      "X: time want lose sense self # # # # # # # # # # # # # # # # #\n",
      "Y: True\n",
      "P: 0.4680637 \n",
      "\n",
      "Training epoch 9\n",
      "Epoch: [9/10], step: [401/2486], loss: 0.120171\n",
      "Epoch: [9/10], step: [801/2486], loss: 0.082584\n",
      "Epoch: [9/10], step: [1201/2486], loss: 0.167884\n",
      "Epoch: [9/10], step: [1601/2486], loss: 0.135420\n",
      "Epoch: [9/10], step: [2001/2486], loss: 0.122505\n",
      "Epoch: [9/10], step: [2401/2486], loss: 0.197634\n",
      "Validation epoch 9 loss: 0.19608104 \n",
      "\n",
      "X: inow bitch getting personali # # # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.04810581 \n",
      "\n",
      "X: s casey # # # # # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.28684047 \n",
      "\n",
      "X: course english pigs # # # # # # # # # # # # # # # # # #\n",
      "Y: False\n",
      "P: 0.087922625 \n",
      "\n",
      "Training epoch 10\n",
      "Epoch: [10/10], step: [401/2486], loss: 0.081920\n",
      "Epoch: [10/10], step: [801/2486], loss: 0.140007\n",
      "Epoch: [10/10], step: [1201/2486], loss: 0.176358\n",
      "Epoch: [10/10], step: [1601/2486], loss: 0.179549\n",
      "Epoch: [10/10], step: [2001/2486], loss: 0.223791\n",
      "Epoch: [10/10], step: [2401/2486], loss: 0.106306\n",
      "Validation epoch 10 loss: 0.15875646 \n",
      "\n",
      "X: unsubscribe time # # # # # # # # # # # # # # # #\n",
      "Y: True\n",
      "P: 0.99960095 \n",
      "\n",
      "X: went drive-thru taco stand interviewed people asking stories # # # # # # # # # #\n",
      "Y: True\n",
      "P: 0.90563077 \n",
      "\n",
      "X: wished dead - s - believe left country another woman - rather sordid # # # # #\n",
      "Y: False\n",
      "P: 0.017574906 \n",
      "\n",
      "\n",
      "Training complete.\n",
      "Epoch: 1, Validation Accuracy: 0.896656\n",
      "Epoch: 2, Validation Accuracy: 0.909363\n",
      "Epoch: 3, Validation Accuracy: 0.917591\n",
      "Epoch: 4, Validation Accuracy: 0.920837\n",
      "Epoch: 5, Validation Accuracy: 0.925417\n",
      "Epoch: 6, Validation Accuracy: 0.926348\n",
      "Epoch: 7, Validation Accuracy: 0.927480\n",
      "Epoch: 8, Validation Accuracy: 0.928839\n",
      "Epoch: 9, Validation Accuracy: 0.929846\n",
      "Epoch: 10, Validation Accuracy: 0.929720\n",
      "Test Accuracy: 0.929514\n"
     ]
    }
   ],
   "source": [
    "exec(open('./intent_classifier.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/loss_accuracy.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/confusion_matrix.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bot Implementation and Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, all pieces to implement the PoliBot are complete! The final component is the actual creation and implementation of the bot. This includes two programs: the core script to run the bot, and a response manager to guide its responses. \n",
    "\n",
    "The response manager incorporates the output of both models described above. The user's query is first passed through the intent classifier to determine either political or chitchat intents. In the case of political intent, the query is then measured against all article embeddings to provide the URLs of its top 3 article recommendations. Otherwise, it is passed to a baseline dialogue model provided by Gunther Cox's ChatterBot package. See the [GitHub](https://github.com/gunthercox/ChatterBot) for details on the ChatterBot's logic and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exec(open('./response_manager.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PoliBot's core script imports the response manager and establishes a BotHandler class to manage its functions. It is initialized using Telegram's API and provided token from BotFather. Once initialized, we're in business!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subprocess.call(['python3', '/.run_bot.py', '--token==597544738:AAFPiA4ejfcpRI20EtbzQCH2-kNCjX1hwIw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the user begins a chat session, the PoliBot will send a welcome message. Importantly, it also allows the user to remove any subset of sources from the recommendation pool at any point during the conversation. As Wall Street Journal articles are only accessible with a subscription, users without a WSJ membership may choose to exclude those articles. The numerical mappings can also be resent at any time with user input \"sources\". This also means that users cannot chat or search for articles by sending solely comma-separated numbers, as it will be interpreted as a search filter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion and Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a fun project to create! While a significant portion of the brunt work is complete, there are a number of improvements that can be made in the future for more effective user-bot interaction:\n",
    "\n",
    "First, and most importantly, right now the article recommendations leave much to be desired. With a limited search domain (some sources pulling from narrow filters such as \"Trump\") and only slight hyperparameter tuning in the Starspace model, the current embeddings yield bot recs that would not stand up in the real world. The current PoliBot should be seen more as prototype than product. :)\n",
    "\n",
    "Beyond corpus size and embeddings, I'd like to spend more time on the dialogue interaction component to improve UX. One obvious next step is to obtain a broader set of dialogues outside of the pre-trained ChatterBot conversations. Training the bot on the Cornell movie dialogues would be helpful. Even better would be data from a substantial Twitter scrape. \n",
    "\n",
    "For longevity's sake, it would be useful to create a bash script that maintains a rolling addition of new articles over time (perhaps once a week).  The current infrastructure makes this straightforward, barring format changes to source websites and their embedded HTML paths.\n",
    "\n",
    "Lastly, the PoliBot could cater more to the idea of debiasing users' political news consumption. There is a growing literature on media bias (e.g. [Budak et al. 2014](http://dx.doi.org/10.2139/ssrn.2526461)). If articles can be accurately classified on the political spectrum, users could potentially specify their political leanings and open-mindedness levels to drive article recommendations. Instead of responding solely based on relevance, the PoliBot could take user profiles into account to more precisely target their news preferences. This would require the daunting task of article classification by political affiliation, which has been proven quite difficult. Still, the potential for this feature to provide more meaningful news consumption is an exciting prospect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
